{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introducción Apache Spark utilizando Pyspark \n","\n","\n","## Importamos SparkSession\n","\n","Las aplicaciones de PySpark comienzan con la inicialización, SparkSession que es el punto de entrada de PySpark como se muestra a continuación. En caso de ejecutarlo en el shell PySpark a través del ejecutable pyspark, el shell crea automáticamente la sesión en la variable spark para los usuarios.\n","\n","*Nota:* Cargar al inicio todos los métodos/modulos que se usarán a lo largo del notebook."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#Importación de modulos\n","\n","from pyspark.sql import SparkSession\n","\n","\n","#Creamos las session de apache spark en una variable\n","\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["Verificamos la versión de apache spark"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-6438-m.us-central1-c.c.iot-pmbh.internal:43021\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.4.8</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7faab821fc50>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#variable de la sesión\n","\n","spark"]},{"cell_type":"markdown","metadata":{},"source":["#### Terminamos la sesión actual\n","No podemos tener mas de una sesión a la vez en nuestro notebook, por lo cual con el método 'stop' terminaremos la applicación.\n","\n","De la misma forma, al terminar una applicación, debemos de indicar explicitamente que termine. De otra forma no liberará los recursos asignados."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","metadata":{},"source":["# RDD (Resilient Distributed Dataset)\n","Un RDD, según Spark, se define como una colección de elementos que es tolerante a fallos y que es capaz de operar en paralelo.\n","Es importante recalcar el tema de que sea capaz de operar en paralelo, porque es la clave o la filosofía básica de Apache Spark.\n","\n","Los RDDs tienen como características principales las siguientes:\n","\n","* Es la principal abstracción de datos, el tipo de dato básico que tiene Apache Spark.\n","* Los RDD están particionados en los distintos nodos del clúster, ya que Apache Spark se suele instalar en un clúster o conjunto de máquinas, por lo que esos RRDs se encuentran distribuidos sobre esas máquinas. Con ello se consigue la tolerancia a fallos, porque si falla una máquina tenemos el fichero en otras máquinas.\n","* Se suelen crear a partir de un fichero del HDFS, el sistema de ficheros distribuidos de Hadoop.\n","* Usan la evaluación perezosa, que consiste en que todas las transformaciones que vayamos haciendo a los RDDs se van a ir almacenando en un DAG y no se van a resolver hasta que no quede más remedio, hasta que la herramienta esté obligada a realizarlas. Esta evaluación perezosa tiene una ventaja y un inconveniente, la primera es que ganamos tiempo, y el inconveniente es que si falla, no lo vamos a ver hasta que se resuelva el grafo completo."]},{"cell_type":"markdown","metadata":{},"source":["#### CREACIÓN DE UN RDD"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#creamos una lista\n","lista = ['Apache Spark', 'Curso de Introducción', 'Big data', 'Cloud']"]},{"cell_type":"markdown","metadata":{},"source":["Para aplicaciones de producción, creamos principalmente RDD utilizando sistemas de almacenamiento externos como HDFS, S3, HBase e.t.c. Para simplificar este tutorial de PySpark RDD, estamos usando archivos del sistema local o cargándolo desde la lista de Python para crear RDD."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#Crear RDD utilizando parallelize   \n","\n","rdd=spark.sparkContext.parallelize(lista)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rdd.collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Crea un RDD vacío usando sparkContext.emptyRDD\n","\n","Usando el emptyRDD()método en sparkContext podemos  crear un RDD sin datos . Este método crea un RDD vacío sin partición."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["rddempty = spark.sparkContext.emptyRDD"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'function' object has no attribute 'collect'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)","\u001B[0;32m<ipython-input-22-554b7bc90321>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrddempty\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'collect'"]}],"source":["rddempty.collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Crea RDD usando sparkContext.wholeTextFiles ()\n","La función wholeTextFiles () devuelve un PairRDD con la clave como la ruta del archivo y el valor como el contenido del archivo."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["rdd = spark.sparkContext.textFile(\"gs://curso-introduccion-apache-spark/datalake/persona.data\")"]},{"cell_type":"markdown","metadata":{},"source":["##### flatMap() \n","transformación aplana el RDD después de aplicar la función y devuelve un nuevo RDD. En el siguiente ejemplo, primero, divide cada registro por espacio en un RDD y finalmente lo aplana. El RDD resultante consta de una sola palabra en cada registro."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["rdd2 = rdd.flatMap(lambda x: x.split(\"|\"))"]},{"cell_type":"markdown","metadata":{},"source":["##### map()\n","transformación se utiliza para aplicar operaciones complejas como agregar una columna, actualizar una columna, etc., la salida de las transformaciones del mapa siempre tendrá el mismo número de registros que la entrada.\n","\n","En nuestro ejemplo de recuento de palabras, estamos agregando una nueva columna con valor 1 para cada palabra, el resultado del RDD es PairRDDFunctionsque contiene pares clave-valor, palabra de tipo Cadena como clave y 1 de tipo Int como valor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","rdd3 = rdd2.map(lambda x: (x,1))"]},{"cell_type":"markdown","metadata":{},"source":["###### reduceByKey()\n","fusiona los valores de cada clave con la función especificada. En nuestro ejemplo, reduce la cadena de palabras aplicando la función de suma en valor. El resultado de nuestro RDD contiene palabras únicas y su recuento. \n","\n","###### sortByKey()\n","transformación se utiliza para ordenar elementos RDD en clave. En nuestro ejemplo, primero, convertimos RDD [(String, Int]) a RDD [(Int, String]) usando la transformación de mapa y aplicamos sortByKey que idealmente ordena en un valor entero. Y finalmente, foreach con declaraciones println devuelve todas las palabras en RDD y su recuento como par clave-valor\n","\n","##### filter() \n","se utiliza para filtrar los registros en un RDD. En nuestro ejemplo, estamos filtrando todas las palabras que comienzan con \"a\".\n","##### first()\n","devuelve el primer registro.\n","##### max()\n","devuelve el registro máximo.\n","\n","##### reduce() \n","reduce los registros a uno solo, podemos usar esto para contar o sumar."]},{"cell_type":"markdown","metadata":{},"source":["## DataFrame\n","\n","Un DataFrame es un DataSet que a la vez está organizado en columnas.\n","Un DataSet es una colección de datos distribuidos que tienen ya una estructura, a diferencia de los RDD, que son conjuntos de datos desestructurados.\n","Vamos a tener los datos estructurados y cada columna con su nombre correspondiente, con lo que nos va a resultar mucho más sencillo consultar, modificar o transformar ese conjunto de datos.\n","\n","Se pueden crear PySpark DataFrame a partir de fuentes de datos como TXT, CSV, JSON, ORV, Avro, Parquet, formatos XML leyendo desde HDFS, S3, DBFS, sistemas de archivos Azure Blob, etc."]},{"cell_type":"markdown","metadata":{},"source":[" ##### Uso de la función toDF ()\n"," l método toDF () de PySpark RDD se utiliza para crear un DataFrame a partir de RDD existente. Dado que RDD no tiene columnas, el DataFrame se crea con los nombres de columna predeterminados “_1” y “_2” ya que tenemos dos columnas."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- _1: string (nullable = true)\n"," |-- _2: string (nullable = true)\n","\n"]}],"source":["data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n","\n","rdd = spark.sparkContext.parallelize(data)\n","\n","\n","#convetir RDD A DATAFRAME\n","dfFromRDD1 = rdd.toDF()\n","\n","#printschema () produce la siguiente salida.\n","dfFromRDD1.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["Si desea proporcionar nombres de columna al DataFrame, utilice el toDF() método con nombres de columna como argumentos, como se muestra a continuación."]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- language: string (nullable = true)\n"," |-- users_count: string (nullable = true)\n","\n"]}],"source":["columns = [\"language\",\"users_count\"]\n","dfFromRDD1 = rdd.toDF(columns)\n","\n","#Esto produce un esquema del DataFrame con nombres de columna.\n","dfFromRDD1.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["##### Usando createDataFrame() desde SparkSession\n","Llamar createDataFrame()desde SparkSession es otra forma de crear PySpark DataFrame manualmente, toma un objeto de lista como argumento. y encadenar con toDF()para especificar nombres a las columnas."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------+\n","|language|users_count|\n","+--------+-----------+\n","|    Java|      20000|\n","|  Python|     100000|\n","|   Scala|       3000|\n","+--------+-----------+\n","\n"]}],"source":["dfFromData2.show()"]},{"cell_type":"markdown","metadata":{},"source":["##### Crear DataFrame con esquema\n","Si desea especificar los nombres de las columnas junto con sus tipos de datos, primero debe crear el esquema StructType y luego asignarlo mientras crea un DataFrame."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: integer (nullable = true)\n","\n","+---------+----------+--------+-----+------+------+\n","|firstname|middlename|lastname|id   |gender|salary|\n","+---------+----------+--------+-----+------+------+\n","|James    |          |Smith   |36636|M     |3000  |\n","|Michael  |Rose      |        |40288|M     |4000  |\n","|Robert   |          |Williams|42114|M     |4000  |\n","|Maria    |Anne      |Jones   |39192|F     |4000  |\n","|Jen      |Mary      |Brown   |     |F     |-1    |\n","+---------+----------+--------+-----+------+------+\n","\n"]}],"source":["from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","\n","data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n","    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n","    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n","    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n","    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n","  ]\n","\n","schema = StructType([ \\\n","    StructField(\"firstname\",StringType(),True), \\\n","    StructField(\"middlename\",StringType(),True), \\\n","    StructField(\"lastname\",StringType(),True), \\\n","    StructField(\"id\", StringType(), True), \\\n","    StructField(\"gender\", StringType(), True), \\\n","    StructField(\"salary\", IntegerType(), True) \\\n","  ])\n"," \n","df = spark.createDataFrame(data=data2,schema=schema)\n","df.printSchema()\n","df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Cree DataFrame a partir de fuentes de datos\n","\n","En tiempo real, la mayoría de las veces crea DataFrame a partir de archivos de origen de datos como CSV, Text, JSON, XML, etc.\n","\n","PySpark de forma predeterminada admite muchos formatos de datos listos para usar sin importar ninguna biblioteca y para crear DataFrame debe usar el método apropiado disponible en la DataFrameReaderclase.\n","\n","##### csv\n","tilice el csv()método del DataFrameReaderobjeto para crear un DataFrame a partir de un archivo CSV. también puede proporcionar opciones como qué delimitador usar, si ha citado datos, formatos de fecha, esquema de inferir\n","\n","<code> df2 = spark.read.csv(\"/src/resources/file.csv\") </code>\n","\n","##### txt\n","De manera similar, también puede crear un DataFrame leyendo un archivo de texto, use el text()método del DataFrameReader para hacerlo.\n","\n","<code> df2 = spark.read.text(\"/src/resources/file.txt\") </code>\n","#####  JSON\n","\n","PySpark también se utiliza para procesar archivos de datos semiestructurados como el formato JSON. puede usar el json()método del DataFrameReader para leer el archivo JSON en DataFrame. A continuación se muestra un ejemplo sencillo.\n","\n","<code> df2 = spark.read.json(\"/src/resources/file.json\") </code>\n","\n","##### Otras fuentes (Avro, Parquet, ORC, Kafka)\n","También podemos crear DataFrame leyendo Avro, Parquet, ORC, archivos binarios y accediendo a la tabla Hive y HBase, y también leyendo datos de Kafka que he explicado en los artículos a continuación, recomendaría leerlos cuando tenga tiempo.\n","\n","<code> parDF1=spark.read.parquet(\"/temp/out/people.parquet\")</code>\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Ejemplo\n","Creando un dataframe desde un csv almacenado en cloud storage "]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import *\n","ruta = 'gs://curso-introduccion-apache-spark/datalake/persona.data'\n","\n","df_schema = StructType([\n","StructField(\"ID\", StringType(),True),\n","StructField(\"NOMBRE\", StringType(),True),\n","StructField(\"TELEFONO\", StringType(),True),\n","StructField(\"CORREO\", StringType(),True),\n","StructField(\"FECHA_INGRESO\", StringType(),True),\n","StructField(\"EDAD\", IntegerType(),True),\n","StructField(\"SALARIO\", DoubleType(),True),\n","StructField(\"ID_EMPRESA\", StringType(),True),\n","])\n","\n","\n","df_with_schema = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(df_schema).load(ruta)"]},{"cell_type":"markdown","metadata":{},"source":["### FUNCIONES\n","### show()\n","muestra el contenido de DataFrame en la tabla.\n","\n","se utiliza para mostrar el contenido del DataFrame en un formato de fila y columna de tabla. De forma predeterminada, muestra solo 20 filas y los valores de las columnas se truncan a los 20 caracteres\n","\n","* Sintaxis\n"," <code> def show(self, n=20, truncate=True, vertical=False) </code>"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+--------------+--------------------+-------------+----+-------+----------+\n","| ID|   NOMBRE|      TELEFONO|              CORREO|FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n","+---+---------+--------------+--------------------+-------------+----+-------+----------+\n","|  2|Priscilla|      155-2498|Donec.egestas.Ali...|   2019-02-17|  34| 9298.0|         2|\n","|  3|  Jocelyn|1-204-956-8594|amet.diam@loborti...|   2002-08-01|  27|10853.0|         3|\n","|  4|    Aidan|1-719-862-9385|euismod.et.commod...|   2018-11-06|  29| 3387.0|        10|\n","|  5|  Leandra|      839-8044|at@pretiumetrutru...|   2002-10-10|  41|22102.0|         1|\n","|  6|     Bert|      797-4453|a.felis.ullamcorp...|   2017-04-25|  70| 7800.0|         7|\n","|  7|     Mark|1-680-102-6792|Quisque.ac@placer...|   2006-04-21|  52| 8112.0|         5|\n","|  8|    Jonah|      214-2975|eu.ultrices.sit@v...|   2017-10-07|  23|17040.0|         5|\n","|  9|    Hanae|      935-2277|          eu@Nunc.ca|   2003-05-25|  69| 6834.0|         3|\n","| 10|   Cadman|1-866-561-2701|orci.adipiscing.n...|   2001-05-19|  19| 7996.0|         7|\n","| 11|  Melyssa|      596-7736|vel@vulputateposu...|   2008-10-14|  48| 4913.0|         8|\n","| 12|   Tanner|1-739-776-7897|arcu.Aliquam.ultr...|   2011-05-10|  24|19943.0|         8|\n","| 13|   Trevor|      512-1955|Nunc.quis.arcu@eg...|   2010-08-06|  34| 9501.0|         5|\n","| 14|    Allen|      733-2795|felis.Donec@necle...|   2005-03-07|  59|16289.0|         2|\n","| 15|    Wanda|      359-6973|Nam.nulla.magna@I...|   2005-08-21|  27| 1539.0|         5|\n","| 16|    Alden|      341-8522|odio@morbitristiq...|   2006-12-05|  26| 3377.0|         2|\n","| 17|     Omar|      720-1543|Phasellus.vitae.m...|   2014-06-24|  60| 6851.0|         6|\n","| 18|     Owen|1-167-335-7541|     sociis@erat.com|   2002-04-09|  34| 4759.0|         7|\n","| 19|    Laura|1-974-623-2057|    mollis@ornare.ca|   2017-03-09|  70|17403.0|         4|\n","| 20|    Emery|1-672-840-0264|     at.nisi@vel.org|   2004-02-27|  24|18752.0|         9|\n","| 21|  Carissa|1-300-877-0859|dignissim.pharetr...|   2011-10-16|  31| 1952.0|        10|\n","+---+---------+--------------+--------------------+-------------+----+-------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["df_with_schema.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### StructType y StructField\n","Las clases PySpark StructType y StructField se utilizan para especificar mediante programación el esquema del DataFrame y crear columnas complejas como estructuras anidadas, matrices y columnas de mapas. StructType es una colección de StructField que define el nombre de la columna, el tipo de datos de la columna, booleano para especificar si el campo puede ser anulable o no y los metadatos.\n","\n","##### StructType()\n","\n","PySpark proporciona desde la pyspark.sql.types import StructTypeclase para definir la estructura del DataFrame.\n","StructType es una colección o lista de objetos StructField.\n","\n","printSchema() El método en el DataFrame muestra las columnas StructType como \"estructura\".\n","\n","##### StructField() \n","define los metadatos de la columna DataFrame.\n","\n","PySpark proporciona una pyspark.sql.types import StructFieldclase para definir las columnas que incluyen el nombre de la columna (String), el tipo de columna ( DataType ), la columna anulable (booleana) y los metadatos (MetaData)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: integer (nullable = true)\n","\n","+---------+----------+--------+-----+------+------+\n","|firstname|middlename|lastname|id   |gender|salary|\n","+---------+----------+--------+-----+------+------+\n","|James    |          |Smith   |36636|M     |3000  |\n","|Michael  |Rose      |        |40288|M     |4000  |\n","|Robert   |          |Williams|42114|M     |4000  |\n","|Maria    |Anne      |Jones   |39192|F     |4000  |\n","|Jen      |Mary      |Brown   |     |F     |-1    |\n","+---------+----------+--------+-----+------+------+\n","\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","\n","spark = SparkSession.builder.master(\"local[1]\") \\\n","                    .appName('SparkByExamples.com') \\\n","                    .getOrCreate()\n","\n","data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n","    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n","    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n","    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n","    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n","  ]\n","\n","schema = StructType([ \\\n","    StructField(\"firstname\",StringType(),True), \\\n","    StructField(\"middlename\",StringType(),True), \\\n","    StructField(\"lastname\",StringType(),True), \\\n","    StructField(\"id\", StringType(), True), \\\n","    StructField(\"gender\", StringType(), True), \\\n","    StructField(\"salary\", IntegerType(), True) \\\n","  ])\n"," \n","df = spark.createDataFrame(data=data,schema=schema)\n","df.printSchema()\n","df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["##### select()\n","función se usa para seleccionar una, varias, columna por índice, todas las columnas de la lista y las columnas anidadas de un DataFrame, PySpark select () \n","Es una función de transformación, por lo que devuelve un nuevo DataFrame con las columnas seleccionadas."]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+-------+-----+\n","|firstname|lastname|country|state|\n","+---------+--------+-------+-----+\n","|James    |Smith   |USA    |CA   |\n","|Michael  |Rose    |USA    |NY   |\n","|Robert   |Williams|USA    |CA   |\n","|Maria    |Jones   |USA    |FL   |\n","+---------+--------+-------+-----+\n","\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n","    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n","    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n","    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n","  ]\n","columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n","df = spark.createDataFrame(data = data, schema = columns)\n","df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["Puede seleccionar una o varias columnas del DataFrame pasando los nombres de columna que desea seleccionar a la select()función. Dado que DataFrame es inmutable, esto crea un nuevo DataFrame con columnas seleccionadas. La función show () se usa para mostrar el contenido del marco de datos.\n","\n","A continuación, se muestran formas de seleccionar una, varias o todas las columnas."]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+\n","|firstname|lastname|\n","+---------+--------+\n","|    James|   Smith|\n","|  Michael|    Rose|\n","|   Robert|Williams|\n","|    Maria|   Jones|\n","+---------+--------+\n","\n","+---------+--------+\n","|firstname|lastname|\n","+---------+--------+\n","|    James|   Smith|\n","|  Michael|    Rose|\n","|   Robert|Williams|\n","|    Maria|   Jones|\n","+---------+--------+\n","\n","+---------+--------+\n","|firstname|lastname|\n","+---------+--------+\n","|    James|   Smith|\n","|  Michael|    Rose|\n","|   Robert|Williams|\n","|    Maria|   Jones|\n","+---------+--------+\n","\n","+---------+--------+\n","|firstname|lastname|\n","+---------+--------+\n","|    James|   Smith|\n","|  Michael|    Rose|\n","|   Robert|Williams|\n","|    Maria|   Jones|\n","+---------+--------+\n","\n","+---------+--------+\n","|firstname|lastname|\n","+---------+--------+\n","|    James|   Smith|\n","|  Michael|    Rose|\n","|   Robert|Williams|\n","|    Maria|   Jones|\n","+---------+--------+\n","\n"]}],"source":["df.select(\"firstname\",\"lastname\").show()\n","\n","df.select(df.firstname,df.lastname).show()\n","\n","df.select(df[\"firstname\"],df[\"lastname\"]).show()\n","\n","#Usando Función col() \n","from pyspark.sql.functions import col\n","df.select(col(\"firstname\"),col(\"lastname\")).show()\n","\n","#seleccionando  columnas  con   expresion regular\n","df.select(df.colRegex(\"`^.*name*`\")).show()"]},{"cell_type":"markdown","metadata":{},"source":["##### withColumn()\n","Es una función de transformación de DataFrame que se utiliza para cambiar el valor, convertir el tipo de datos de una columna existente, crear una nueva columna y muchos más. En esta publicación, lo guiaré a través de las operaciones de columna de PySpark DataFrame de uso común usando ejemplos withColumn ()."]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["\n","data = [('James','','Smith','1991-04-01','M',3000),\n","  ('Michael','Rose','','2000-05-19','M',4000),\n","  ('Robert','','Williams','1978-09-05','M',4000),\n","  ('Maria','Anne','Jones','1967-12-01','F',4000),\n","  ('Jen','Mary','Brown','1980-02-17','F',-1)\n","]\n","\n","columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","df = spark.createDataFrame(data=data, schema = columns)"]},{"cell_type":"markdown","metadata":{},"source":["withColumn() en un DataFrame, podemos convertir o cambiar el tipo de datos de una columna. Para cambiar el tipo de datos , también necesitaría usar la cast()función junto con withColumn (). La siguiente declaración cambia el tipo de datos de StringaInteger para la salarycolumna."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\n","\n","#Actualizar el valor existente\n","\n","df.withColumn(\"salary\",col(\"salary\")*100).show()\n","\n","#Crear una nueva columna a partir de una existente\n","\n","df.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n","\n","#Renombrar una columna\n","df.withColumnRenamed(\"gender\",\"sex\") \\\n","  .show(truncate=False) \n","\n","#Eliminar una columna\n","\n","df.drop(\"salary\").show() "]},{"cell_type":"markdown","metadata":{},"source":["#### EJEMPLO COMPLETO"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n","+---------+----------+--------+----------+------+------+\n","|firstname|middlename|lastname|dob       |gender|salary|\n","+---------+----------+--------+----------+------+------+\n","|James    |          |Smith   |1991-04-01|M     |3000  |\n","|Michael  |Rose      |        |2000-05-19|M     |4000  |\n","|Robert   |          |Williams|1978-09-05|M     |4000  |\n","|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n","|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n","+---------+----------+--------+----------+------+------+\n","\n","root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: integer (nullable = true)\n","\n","+---------+----------+--------+----------+------+------+\n","|firstname|middlename|lastname|dob       |gender|salary|\n","+---------+----------+--------+----------+------+------+\n","|James    |          |Smith   |1991-04-01|M     |3000  |\n","|Michael  |Rose      |        |2000-05-19|M     |4000  |\n","|Robert   |          |Williams|1978-09-05|M     |4000  |\n","|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n","|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n","+---------+----------+--------+----------+------+------+\n","\n","root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n","+---------+----------+--------+----------+------+------+\n","|firstname|middlename|lastname|dob       |gender|salary|\n","+---------+----------+--------+----------+------+------+\n","|James    |          |Smith   |1991-04-01|M     |300000|\n","|Michael  |Rose      |        |2000-05-19|M     |400000|\n","|Robert   |          |Williams|1978-09-05|M     |400000|\n","|Maria    |Anne      |Jones   |1967-12-01|F     |400000|\n","|Jen      |Mary      |Brown   |1980-02-17|F     |-100  |\n","+---------+----------+--------+----------+------+------+\n","\n","root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n"," |-- CopiedColumn: long (nullable = true)\n","\n","root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n"," |-- Country: string (nullable = false)\n","\n","root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n"," |-- Country: string (nullable = false)\n"," |-- anotherColumn: string (nullable = false)\n","\n","+---------+----------+--------+----------+---+------+\n","|firstname|middlename|lastname|dob       |sex|salary|\n","+---------+----------+--------+----------+---+------+\n","|James    |          |Smith   |1991-04-01|M  |3000  |\n","|Michael  |Rose      |        |2000-05-19|M  |4000  |\n","|Robert   |          |Williams|1978-09-05|M  |4000  |\n","|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n","|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n","+---------+----------+--------+----------+---+------+\n","\n","+---------+----------+--------+----------+------+------+\n","|firstname|middlename|lastname|dob       |gender|salary|\n","+---------+----------+--------+----------+------+------+\n","|James    |          |Smith   |1991-04-01|M     |3000  |\n","|Michael  |Rose      |        |2000-05-19|M     |4000  |\n","|Robert   |          |Williams|1978-09-05|M     |4000  |\n","|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n","|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n","+---------+----------+--------+----------+------+------+\n","\n"]}],"source":["\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, lit\n","from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","data = [('James','','Smith','1991-04-01','M',3000),\n","  ('Michael','Rose','','2000-05-19','M',4000),\n","  ('Robert','','Williams','1978-09-05','M',4000),\n","  ('Maria','Anne','Jones','1967-12-01','F',4000),\n","  ('Jen','Mary','Brown','1980-02-17','F',-1)\n","]\n","\n","columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n","df = spark.createDataFrame(data=data, schema = columns)\n","df.printSchema()\n","df.show(truncate=False)\n","\n","df2 = df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))\n","df2.printSchema()\n","df2.show(truncate=False)\n","\n","df3 = df.withColumn(\"salary\",col(\"salary\")*100)\n","df3.printSchema()\n","df3.show(truncate=False) \n","\n","df4 = df.withColumn(\"CopiedColumn\",col(\"salary\")* -1)\n","df4.printSchema()\n","\n","df5 = df.withColumn(\"Country\", lit(\"USA\"))\n","df5.printSchema()\n","\n","df6 = df.withColumn(\"Country\", lit(\"USA\")) \\\n","   .withColumn(\"anotherColumn\",lit(\"anotherValue\"))\n","df6.printSchema()\n","\n","df.withColumnRenamed(\"gender\",\"sex\") \\\n","  .show(truncate=False) \n","  \n","df4.drop(\"CopiedColumn\") \\\n",".show(truncate=False) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}